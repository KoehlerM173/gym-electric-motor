{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an RL agent with Keras-RL2 Using a GEM Environment\n",
    "\n",
    "This notebook serves as an educational introduction to the usage of Keras-RL2 using a GEM environment. Goal of this notebook is to give an understanding of what Keras-RL2 is and how to use it to train and evaluate a Reinforcement Learning agent which is able to solve a current control problem within the GEM toolbox."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can start you need to make sure that you have installed both, gym-electric-motor and Keras-RL2. You can install both easily using pip:\n",
    "\n",
    "- ```pip install gym-electric-motor```\n",
    "- ```pip install keras-rl2```\n",
    "\n",
    "Alternatively, you can install them and their latest developer version directly from GitHub (recommended) :\n",
    "\n",
    "- https://github.com/upb-lea/gym-electric-motor\n",
    "- https://github.com/wau/keras-rl2\n",
    "\n",
    "For this notebook, the following cell will do the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/upb-lea/gym-electric-motor.git git+https://github.com/wau/keras-rl2.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up a GEM Environment\n",
    "\n",
    "The basic idea behind reinforcement learning is to create a so-called agent, that should learn by itself to solve a specified task in a given environment. \n",
    "This environment gives the agent feedback on its actions and reinforces the targeted behavior.\n",
    "In this notebook, the task is to train a controller for the current control of a *permanent magnet synchronous motor* (*PMSM*).\n",
    " \n",
    "In the following, the used GEM-environment is briefly presented, but this notebook does not focus directly on the detailed usage of GEM. If you are new to the used environment and interested in finding out what it does and how to use it, you should take a look at the [GEM cookbook](https://colab.research.google.com/github/upb-lea/gym-electric-motor/blob/master/examples/example_notebooks/GEM_cookbook.ipynb).\n",
    "\n",
    "The basic idea of the control setup from the GEM-environment is displayed in the following figure. \n",
    "\n",
    "![](../../docs/plots/SCML_Overview.png)\n",
    "\n",
    "The agent controls the converter who converts the supply currents to the currents flowing into the motor - for the *PMSM*: $i_{sq}$ and $i_{sd}$\n",
    "\n",
    "In the continuous case, the agent's action equals a duty cycle which will be modulated into a corresponding voltage. \n",
    "\n",
    "In the discrete case, the agent's actions denote switching states of the converter at the given instant. Here, only a discrete amount of options are available. In this notebook, for the PMSM the *discrete B6 bridge converter* with six switches is utilized per default. This converter provides a total of eight possible actions.\n",
    "\n",
    "![](../../docs/plots/B6.svg)\n",
    "\n",
    "The motor schematic is the following:\n",
    "\n",
    "\n",
    "![](../../docs/plots/ESBdq.svg)\n",
    "\n",
    "And the electrical ODEs for that motor are:\n",
    "\n",
    "<h3 align=\"center\">\n",
    "\n",
    "<!-- $\\frac{\\mathrm{d}i_{sq}}{\\mathrm{d}t} = \\frac{u_{sq}-pL_d\\omega_{me}i_{sd}-R_si_{sq}}{L_q}$\n",
    "\n",
    "$\\frac{\\mathrm{d}i_{sd}}{\\mathrm{d}t} = \\frac{u_{sd}-pL_q\\omega_{me}i_{sq}-R_si_{sd}}{L_d}$\n",
    "\n",
    "$\\frac{\\mathrm{d}\\epsilon_{el}}{\\mathrm{d}t} = p\\omega_{me}$\n",
    " -->\n",
    "\n",
    "   $ \\frac{\\mathrm{d}i_{sd}}{\\mathrm{d}t}=\\frac{u_{sd} + p\\omega_{me}L_q i_{sq} - R_s i_{sd}}{L_d} $ <br><br>\n",
    "    $\\frac{\\mathrm{d} i_{sq}}{\\mathrm{d} t}=\\frac{u_{sq} - p \\omega_{me} (L_d i_{sd} + \\mathit{\\Psi}_p) - R_s i_{sq}}{L_q}$ <br><br>\n",
    "   $\\frac{\\mathrm{d}\\epsilon_{el}}{\\mathrm{d}t} = p\\omega_{me}$\n",
    "\n",
    "</h3>\n",
    "\n",
    "The target for the agent is now to learn to control the currents. For this, a reference generator produces a trajectory that the agent has to follow. \n",
    "Therefore, it has to learn a function (policy) from given states, references and rewards to appropriate actions.\n",
    "\n",
    "For a deeper understanding of the used models behind the environment see the [documentation](https://upb-lea.github.io/gym-electric-motor/).\n",
    "Comprehensive learning material to RL is also [freely available](https://github.com/upb-lea/reinforcement_learning_course_materials).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arnet\\anaconda3\\envs\\gem_dev\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import gym_electric_motor as gem\n",
    "from gym_electric_motor.reference_generators import \\\n",
    "    MultipleReferenceGenerator,\\\n",
    "    WienerProcessReferenceGenerator\n",
    "from gym_electric_motor.visualization import MotorDashboard\n",
    "from gym.spaces import Discrete, Box\n",
    "from gym.wrappers import FlattenObservation, TimeLimit\n",
    "from gym import ObservationWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureWrapper(ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Wrapper class which wraps the environment to change its observation. Serves\n",
    "    the purpose to improve the agent's learning speed.\n",
    "    \n",
    "    It changes epsilon to cos(epsilon) and sin(epsilon). This serves the purpose\n",
    "    to have the angles -pi and pi close to each other numerically without losing\n",
    "    any information on the angle.\n",
    "    \n",
    "    Additionally, this wrapper adds a new observation i_sd**2 + i_sq**2. This should\n",
    "    help the agent to easier detect incoming limit violations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, epsilon_idx, i_sd_idx, i_sq_idx):\n",
    "        \"\"\"\n",
    "        Changes the observation space to fit the new features\n",
    "        \n",
    "        Args:\n",
    "            env(GEM env): GEM environment to wrap\n",
    "            epsilon_idx(integer): Epsilon's index in the observation array\n",
    "            i_sd_idx(integer): I_sd's index in the observation array\n",
    "            i_sq_idx(integer): I_sq's index in the observation array\n",
    "        \"\"\"\n",
    "        super(FeatureWrapper, self).__init__(env)\n",
    "        self.EPSILON_IDX = epsilon_idx\n",
    "        self.I_SQ_IDX = i_sq_idx\n",
    "        self.I_SD_IDX = i_sd_idx\n",
    "        new_low = np.concatenate((self.env.observation_space.low[\n",
    "                                  :self.EPSILON_IDX], np.array([-1.]),\n",
    "                                  self.env.observation_space.low[\n",
    "                                  self.EPSILON_IDX:], np.array([0.])))\n",
    "        new_high = np.concatenate((self.env.observation_space.high[\n",
    "                                   :self.EPSILON_IDX], np.array([1.]),\n",
    "                                   self.env.observation_space.high[\n",
    "                                   self.EPSILON_IDX:],np.array([1.])))\n",
    "\n",
    "        self.observation_space = Box(new_low, new_high)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        \"\"\"\n",
    "        Gets called at each return of an observation. Adds the new features to the\n",
    "        observation and removes original epsilon.\n",
    "        \n",
    "        \"\"\"\n",
    "        cos_eps = np.cos(observation[self.EPSILON_IDX] * np.pi)\n",
    "        sin_eps = np.sin(observation[self.EPSILON_IDX] * np.pi)\n",
    "        currents_squared = observation[self.I_SQ_IDX]**2 + observation[self.I_SD_IDX]**2\n",
    "        observation = np.concatenate((observation[:self.EPSILON_IDX],\n",
    "                                      np.array([cos_eps, sin_eps]),\n",
    "                                      observation[self.EPSILON_IDX + 1:],\n",
    "                                      np.array([currents_squared])))\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arnet\\anaconda3\\envs\\gem_dev\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# define motor arguments\n",
    "motor_parameter = dict(\n",
    "    p=3,  # [p] = 1, nb of pole pairs\n",
    "    r_s=17.932e-3,  # [r_s] = Ohm, stator resistance\n",
    "    l_d=0.37e-3,  # [l_d] = H, d-axis inductance\n",
    "    l_q=1.2e-3,  # [l_q] = H, q-axis inductance\n",
    "    psi_p=65.65e-3,  # [psi_p] = Vs, magnetic flux of the permanent magnet\n",
    ")\n",
    "# supply voltage\n",
    "u_supply = 350\n",
    "\n",
    "# nominal and absolute state limitations\n",
    "nominal_values=dict(\n",
    "    omega=4000*2*np.pi/60,\n",
    "    i=230,\n",
    "    u=u_supply\n",
    ")\n",
    "limit_values=dict(\n",
    "    omega=4000*2*np.pi/60,\n",
    "    i=1.5*230,\n",
    "    u=u_supply\n",
    ")\n",
    "\n",
    "# sampling interval\n",
    "tau = 1e-5\n",
    "\n",
    "# define maximal episode steps\n",
    "max_eps_steps = 10000\n",
    "\n",
    "\n",
    "motor_initializer={'random_init': 'uniform', 'interval': [[-230, 230], [-230, 230], [-np.pi, np.pi]]}\n",
    "reward_function=gem.reward_functions.WeightedSumOfErrors(\n",
    "    reward_weights={'i_sq': 10, 'i_sd': 10},\n",
    "    gamma=0.99,  # discount rate \n",
    "    reward_power=1\n",
    ")\n",
    "\n",
    "\n",
    "# creating gem environment\n",
    "env = gem.make(  # define a PMSM with discrete action space\n",
    "    \"Finite-CC-PMSM-v0\",\n",
    "    # visualize the results\n",
    "    visualization=MotorDashboard(state_plots=['i_sq', 'i_sd'], reward_plot=True),\n",
    "    \n",
    "    # parameterize the PMSM and update limitations\n",
    "    motor=dict(\n",
    "        motor_parameter=motor_parameter,\n",
    "        limit_values=limit_values,\n",
    "        nominal_values=nominal_values,\n",
    "        motor_initializer=motor_initializer,\n",
    "    ),\n",
    "    # define the random initialisation for load and motor\n",
    "    load=dict(\n",
    "        load_initializer={'random_init': 'uniform', },\n",
    "    ),\n",
    "    reward_function=reward_function,\n",
    "    supply=dict(u_sup=u_supply),\n",
    "    # define the duration of one sampling step\n",
    "    tau=tau,\n",
    "    \n",
    "    ode_solver='euler',\n",
    ")\n",
    "\n",
    "# remove one action from the action space to help the agent speed up its training\n",
    "# this can be done as both switchting states (1,1,1) and (-1,-1,-1) - which are encoded\n",
    "# by action 0 and 7 - both lead to the same zero voltage vector in alpha/beta-coordinates\n",
    "env.action_space = Discrete(7)\n",
    "\n",
    "# applying wrappers\n",
    "eps_idx = env.physical_system.state_names.index('epsilon')\n",
    "i_sd_idx = env.physical_system.state_names.index('i_sd')\n",
    "i_sq_idx = env.physical_system.state_names.index('i_sq')\n",
    "env = TimeLimit(\n",
    "    FeatureWrapper(\n",
    "        FlattenObservation(env), \n",
    "        eps_idx, i_sd_idx, i_sq_idx\n",
    "    ),\n",
    "    max_eps_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train an Agent with Keras-RL2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Keras-RL2](https://github.com/wau/keras-rl2) is a widely used Python library for reinforcement learning. It compiles a collection of Reinforcement Learning algorithm implementations.\n",
    "Most of the algorithms and their extensions are readily available and are quite stable. The ease of adopting to various environments adds to its popularity.\n",
    "For currently available RL algorithms see their [documentation](https://keras-rl.readthedocs.io/en/latest/agents/overview/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Imports\n",
    "The environment in this control problem poses a discrete action space. Therefore, the [Deep Q-Network (DQN)](https://arxiv.org/abs/1312.5602) is a suitable agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Parameters and Instantiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the DQN algorithm, a set of parameters has to be pre-defined.\n",
    "In particular, a multilayer perceptron (MLP) is used as function approximator within the DQN algorithm.\n",
    "Note: Although there are 8 possible actions, we ignore the last action as the voltage applied on the motor there is the same as for the first action. This effectively reduces the feature space that is to be explored and benefits training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters.\n",
    "time_limit = True\n",
    "buffer_size = 200000  # observation history size\n",
    "batch_size = 25  # mini batch size sampled from history at each update step\n",
    "nb_actions = env.action_space.n\n",
    "window_length = 1\n",
    "\n",
    "# construct a MLP\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(window_length,) + env.observation_space.shape))\n",
    "model.add(Dense(64, activation='relu'))  # hidden layer 1\n",
    "model.add(Dense(64, activation='relu'))  # hidden layer 2\n",
    "model.add(Dense(nb_actions, activation='linear'))  # output layer\n",
    "\n",
    "# keras-rl2 objects\n",
    "memory = SequentialMemory(limit=200000, window_length=window_length)\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(eps=0.2), 'eps', 1, 0.05, 0, 50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've setup the environment and defined your parameters starting the training is nothing more than a one-liner. For each algorithm all you have to do is call its ```fit()``` function.\n",
    "\n",
    "The advantage of keras-RL2 is that you can see all the important parameters and their values during training phase to get an intution of learning.\n",
    "Hence, there is no need of additional callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(\n",
    "    model=model,\n",
    "    policy=policy,\n",
    "    nb_actions=nb_actions,\n",
    "    memory=memory,\n",
    "    gamma=0.99,\n",
    "    batch_size=25,\n",
    "    train_interval=1,\n",
    "    memory_interval=1,\n",
    "    target_model_update=1000,\n",
    "    nb_steps_warmup=10000,\n",
    "    enable_double_dqn=True\n",
    ")\n",
    "\n",
    "dqn.compile(\n",
    "    Adam(lr=1e-4),\n",
    "    metrics=['mse']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FiniteCurrentControlPermanentMagnetSynchronousMoto' object has no attribute 'visualization'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-976ec947befb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'notebook'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_figures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m history = dqn.fit(\n\u001b[0;32m      4\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mnb_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gem_dev\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"attempted to get missing private attribute '{}'\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gem_dev\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"attempted to get missing private attribute '{}'\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gem_dev\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"attempted to get missing private attribute '{}'\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FiniteCurrentControlPermanentMagnetSynchronousMoto' object has no attribute 'visualization'"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "env.visualization.reset_figures()\n",
    "history = dqn.fit(\n",
    "    env,\n",
    "    nb_steps=100000,\n",
    "    action_repetition=1,\n",
    "    verbose=2,\n",
    "    visualize=True,\n",
    "    nb_max_episode_steps=10000,\n",
    "    log_interval=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Saving the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the training is finished, you can save the model that your DQN-agent has learned for later reuse, e.g. for evaluation or continued training. For this purpose, ```save_weights()``` is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = Path('saved_agents')\n",
    "weight_path.mkdir(parents=True, exist_ok=True)\n",
    "dqn.save_weights(str(weight_path / 'dqn_keras-RL2.hdf5'), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating an Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras-RL2 agents have their own ```test()``` method which can be used to evaluate their performance.\n",
    "Here, the metric used is the total reward the agent accumulates at the end of each episode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Loading a Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dqn.load_weights(str(weight_path / 'dqn_keras-RL2.hdf5'))\n",
    "except OSError:\n",
    "    print('Could not find model file. Continue')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -44281.266, steps: 10000\n",
      "Episode 2: reward: -31202.600, steps: 10000\n",
      "Episode 3: reward: -27821.012, steps: 10000\n",
      "Episode 4: reward: -24366.201, steps: 10000\n",
      "Episode 5: reward: -30157.671, steps: 10000\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "test_history = dqn.test(env,\n",
    "                         nb_episodes=5,\n",
    "                         nb_max_episode_steps=100000,\n",
    "                         visualize=True\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
